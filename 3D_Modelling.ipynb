{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak07_hQWU22L"
   },
   "source": [
    "# Step 1: Setting up the Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CSE57ualUTgD",
    "outputId": "7b522602-0937-4cb0-94a2-339d2e203c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision matplotlib numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzIa0DH_U-Gz"
   },
   "source": [
    "# Step 2: Importing necessary libraries and setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-XEMpPQxUX7C"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7PyJEqdVB_C"
   },
   "source": [
    "# Step 3: Implementing the NeRF model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "p-LFEDToUZDK",
    "outputId": "cd973313-0c1c-4f8f-d10b-296cd23bb5bf"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_frequencies, d_in=3):\n",
    "        super().__init__()\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.d_in = d_in\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = (2.0 ** torch.arange(self.num_frequencies, device=x.device)).view(1, -1) * x.unsqueeze(-1)\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1).flatten(start_dim=-2)\n",
    "\n",
    "class NeRF(nn.Module):\n",
    "    def __init__(self, d_in=3, d_hidden=256, d_out=4, num_layers=8, skip_layers=[4], num_frequencies=10):\n",
    "        super().__init__()\n",
    "        self.positional_encoding = PositionalEncoding(num_frequencies, d_in)\n",
    "        d_in_encoded = d_in * num_frequencies * 2\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(d_in_encoded, d_hidden))\n",
    "\n",
    "        for i in range(1, num_layers):\n",
    "            if i in skip_layers:\n",
    "                self.layers.append(nn.Linear(d_hidden + d_in_encoded, d_hidden))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(d_hidden, d_hidden))\n",
    "\n",
    "        self.output_layer = nn.Linear(d_hidden, d_out)\n",
    "        self.skip_layers = skip_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_encoded = self.positional_encoding(x)\n",
    "        h = x_encoded\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i in self.skip_layers:\n",
    "                h = torch.cat([h, x_encoded], dim=-1)\n",
    "            h = F.relu(layer(h))\n",
    "\n",
    "        output = self.output_layer(h)\n",
    "        rgb = torch.sigmoid(output[..., :3])\n",
    "        sigma = F.relu(output[..., 3])\n",
    "        return rgb, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h00K8oloVGZE"
   },
   "source": [
    "# Step 4: Loading and preprocessing 2D image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "W2EP3nyMUbya",
    "outputId": "5630a62c-ca82-4315-bf5a-896374ce59a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2181080/2371415632.py:10: RuntimeWarning: invalid value encountered in sqrt\n",
      "  zz = np.sqrt(1 - xx**2 - yy**2)\n"
     ]
    }
   ],
   "source": [
    "def generate_synthetic_data(num_images=100, image_size=32):\n",
    "    images = []\n",
    "    poses = []\n",
    "\n",
    "    for _ in range(num_images):\n",
    "        # Generate a simple sphere\n",
    "        x = np.linspace(-1, 1, image_size)\n",
    "        y = np.linspace(-1, 1, image_size)\n",
    "        xx, yy = np.meshgrid(x, y)\n",
    "        zz = np.sqrt(1 - xx**2 - yy**2)\n",
    "\n",
    "        # Add some noise\n",
    "        zz += np.random.normal(0, 0.1, zz.shape)\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        zz = (zz - zz.min()) / (zz.max() - zz.min())\n",
    "\n",
    "        images.append(zz)\n",
    "\n",
    "        # Generate random camera poses\n",
    "        pose = np.eye(4)\n",
    "        pose[:3, 3] = np.random.uniform(-1, 1, 3)\n",
    "        poses.append(pose)\n",
    "\n",
    "    return np.array(images), np.array(poses)\n",
    "\n",
    "# Generate synthetic data\n",
    "images, poses = generate_synthetic_data()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "images = torch.from_numpy(images).float().to(device)\n",
    "poses = torch.from_numpy(poses).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JobwpVa6VOjF"
   },
   "source": [
    "# Step 5: Implementing the forward pass and ray sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Coyl7uMmUjg0"
   },
   "outputs": [],
   "source": [
    "def get_rays(H, W, focal, c2w):\n",
    "    i, j = torch.meshgrid(torch.linspace(0, W-1, W), torch.linspace(0, H-1, H))\n",
    "    i = i.t()\n",
    "    j = j.t()\n",
    "    dirs = torch.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -torch.ones_like(i)], -1)\n",
    "    rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
    "    rays_o = c2w[:3,-1].expand(rays_d.shape)\n",
    "    return rays_o, rays_d\n",
    "\n",
    "def sample_pdf(bins, weights, N_samples, det=False):\n",
    "    weights = weights + 1e-5\n",
    "    pdf = weights / torch.sum(weights, -1, keepdim=True)\n",
    "    cdf = torch.cumsum(pdf, -1)\n",
    "    cdf = torch.cat([torch.zeros_like(cdf[...,:1]), cdf], -1)\n",
    "\n",
    "    if det:\n",
    "        u = torch.linspace(0., 1., steps=N_samples)\n",
    "        u = u.expand(list(cdf.shape[:-1]) + [N_samples])\n",
    "    else:\n",
    "        u = torch.rand(list(cdf.shape[:-1]) + [N_samples])\n",
    "\n",
    "    u = u.to(weights.device)\n",
    "    inds = torch.searchsorted(cdf, u, right=True)\n",
    "    below = torch.max(torch.zeros_like(inds-1), inds-1)\n",
    "    above = torch.min((cdf.shape[-1]-1) * torch.ones_like(inds), inds)\n",
    "    inds_g = torch.stack([below, above], -1)\n",
    "\n",
    "    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n",
    "    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n",
    "    bins_g = torch.gather(bins.unsqueeze(1).expand(matched_shape), 2, inds_g)\n",
    "\n",
    "    denom = (cdf_g[...,1]-cdf_g[...,0])\n",
    "    denom = torch.where(denom<1e-5, torch.ones_like(denom), denom)\n",
    "    t = (u-cdf_g[...,0])/denom\n",
    "    samples = bins_g[...,0] + t * (bins_g[...,1]-bins_g[...,0])\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tgg4H8U2VUv3"
   },
   "source": [
    "# Step 6: Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "opoSPgoaUohu"
   },
   "outputs": [],
   "source": [
    "def render_rays(ray_batch, network_fn, N_samples=64, perturb=0, N_importance=0, raw_noise_std=0):\n",
    "    N_rays = ray_batch.shape[0]\n",
    "    rays_o, rays_d = ray_batch[:,0:3], ray_batch[:,3:6]\n",
    "    near, far = ray_batch[:,6:7], ray_batch[:,7:8]\n",
    "\n",
    "    t_vals = torch.linspace(0., 1., steps=N_samples)\n",
    "    z_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "\n",
    "    if perturb > 0.:\n",
    "        mids = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        upper = torch.cat([mids, z_vals[...,-1:]], -1)\n",
    "        lower = torch.cat([z_vals[...,:1], mids], -1)\n",
    "        t_rand = torch.rand(z_vals.shape)\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "\n",
    "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[\n",
    "    ...,:,None]\n",
    "\n",
    "    raw = network_fn(pts)\n",
    "    rgb, sigma = raw[...,:-1], raw[...,-1]\n",
    "\n",
    "    dists = z_vals[...,1:] - z_vals[...,:-1]\n",
    "    dists = torch.cat([dists, torch.Tensor([1e10]).expand(dists[...,:1].shape)], -1)\n",
    "\n",
    "    alpha = 1. - torch.exp(-sigma * dists)\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)), 1.-alpha + 1e-10], -1), -1)[:, :-1]\n",
    "\n",
    "    rgb_map = torch.sum(weights[...,None] * rgb, -2)\n",
    "    depth_map = torch.sum(weights * z_vals, -1)\n",
    "    acc_map = torch.sum(weights, -1)\n",
    "\n",
    "    if N_importance > 0:\n",
    "        z_vals_mid = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        z_samples = sample_pdf(z_vals_mid, weights[...,1:-1], N_importance, det=(perturb==0.))\n",
    "        z_samples = z_samples.detach()\n",
    "\n",
    "        z_vals, _ = torch.sort(torch.cat([z_vals, z_samples], -1), -1)\n",
    "        pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
    "\n",
    "        raw = network_fn(pts)\n",
    "        rgb, sigma = raw[...,:-1], raw[...,-1]\n",
    "\n",
    "        dists = z_vals[...,1:] - z_vals[...,:-1]\n",
    "        dists = torch.cat([dists, torch.Tensor([1e10]).expand(dists[...,:1].shape)], -1)\n",
    "\n",
    "        alpha = 1. - torch.exp(-sigma * dists)\n",
    "        weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)), 1.-alpha + 1e-10], -1), -1)[:, :-1]\n",
    "\n",
    "        rgb_map = torch.sum(weights[...,None] * rgb, -2)\n",
    "        depth_map = torch.sum(weights * z_vals, -1)\n",
    "        acc_map = torch.sum(weights, -1)\n",
    "\n",
    "    return rgb_map, depth_map, acc_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgEL740WVZ74"
   },
   "source": [
    "# Step 7: Evaluation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "q8iWfyw2Usl0",
    "outputId": "a5344fc3-0c9b-4443-a2ab-cecc0e4400f5"
   },
   "outputs": [],
   "source": [
    "def train(images, poses, H, W, focal, num_epochs=100, batch_size=1024, lr=5e-4):\n",
    "    nerf = NeRF().to(device)\n",
    "    optimizer = torch.optim.Adam(nerf.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for img_i in range(images.shape[0]):\n",
    "            img = images[img_i]\n",
    "            pose = poses[img_i]\n",
    "\n",
    "            rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "            rays_o = rays_o.reshape(-1, 3)\n",
    "            rays_d = rays_d.reshape(-1, 3)\n",
    "\n",
    "            select_inds = np.random.choice(rays_o.shape[0], size=[batch_size], replace=False)\n",
    "            rays_o = rays_o[select_inds]\n",
    "            rays_d = rays_d[select_inds]\n",
    "\n",
    "            target_s = img[select_inds]\n",
    "\n",
    "            rays_o = rays_o.to(device)\n",
    "            rays_d = rays_d.to(device)\n",
    "            target_s = target_s.to(device)\n",
    "\n",
    "            near = 0. * torch.ones_like(rays_d[...,:1])\n",
    "            far = 1. * torch.ones_like(rays_d[...,:1])\n",
    "\n",
    "            rays = torch.cat([rays_o, rays_d, near, far], -1)\n",
    "\n",
    "            rgb, depth, acc = render_rays(rays, nerf)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.mse_loss(rgb, target_s)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    return nerf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 546 is out of bounds for dimension 0 with size 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m H, W \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      3\u001b[0m focal \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m W \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mtan(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# assume 60° FOV\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m trained_nerf \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfocal\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(images, poses, H, W, focal, num_epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     15\u001b[0m rays_o \u001b[38;5;241m=\u001b[39m rays_o[select_inds]\n\u001b[1;32m     16\u001b[0m rays_d \u001b[38;5;241m=\u001b[39m rays_d[select_inds]\n\u001b[0;32m---> 18\u001b[0m target_s \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselect_inds\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m rays_o \u001b[38;5;241m=\u001b[39m rays_o\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m rays_d \u001b[38;5;241m=\u001b[39m rays_d\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 546 is out of bounds for dimension 0 with size 32"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "H, W = images.shape[1], images.shape[2]\n",
    "focal = 0.5 * W / np.tan(0.5 * np.pi / 3)  # assume 60° FOV\n",
    "trained_nerf = train(images, poses, H, W, focal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "X17X5G_dUv3I",
    "outputId": "438cc060-1941-4f77-9d7d-853ca8b2fcbc"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_nerf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m novel_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m novel_pose[:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 25\u001b[0m rgb, depth \u001b[38;5;241m=\u001b[39m render_novel_view(\u001b[43mtrained_nerf\u001b[49m, H, W, focal, novel_pose)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Visualize the results\u001b[39;00m\n\u001b[1;32m     28\u001b[0m fig, (ax1, ax2) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_nerf' is not defined"
     ]
    }
   ],
   "source": [
    "def render_novel_view(nerf, H, W, focal, pose):\n",
    "    rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "    rays_o = rays_o.reshape(-1, 3)\n",
    "    rays_d = rays_d.reshape(-1, 3)\n",
    "\n",
    "    rays_o = rays_o.to(device)\n",
    "    rays_d = rays_d.to(device)\n",
    "\n",
    "    near = 0. * torch.ones_like(rays_d[...,:1])\n",
    "    far = 1. * torch.ones_like(rays_d[...,:1])\n",
    "\n",
    "    rays = torch.cat([rays_o, rays_d, near, far], -1)\n",
    "\n",
    "    rgb, depth, acc = render_rays(rays, nerf)\n",
    "\n",
    "    rgb = rgb.reshape(H, W, 3).cpu().detach().numpy()\n",
    "    depth = depth.reshape(H, W).cpu().detach().numpy()\n",
    "\n",
    "    return rgb, depth\n",
    "\n",
    "# Generate a novel view\n",
    "novel_pose = torch.eye(4).to(device)\n",
    "novel_pose[:3, 3] = torch.tensor([0.5, 0.5, -2.0]).to(device)\n",
    "\n",
    "rgb, depth = render_novel_view(trained_nerf, H, W, focal, novel_pose)\n",
    "\n",
    "# Visualize the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.imshow(rgb)\n",
    "ax1.set_title(\"RGB\")\n",
    "ax2.imshow(depth, cmap='viridis')\n",
    "ax2.set_title(\"Depth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8kR_nFhUxLh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Class-Pulse",
   "language": "python",
   "name": "class-pulse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
